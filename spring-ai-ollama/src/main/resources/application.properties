spring.application.name=spring-ai-ollama
# Base URL for Ollama running locally
spring.ai.ollama.base-url=http://localhost:11434
# Model configuration: using mistral
spring.ai.ollama.chat.options.model=mistral
# Optional: adjust temperature for response creativity (0.0 - 1.0)
spring.ai.ollama.chat.options.temperature=0.7


